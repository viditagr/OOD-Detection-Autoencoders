{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gIElO8-NyWWL"},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"TOOJW5vYqleG"},"source":["Mounting the google drive and making sure we have the right stuff!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24500,"status":"ok","timestamp":1685774846080,"user":{"displayName":"VIDIT AGRAWAL","userId":"17500259214364609360"},"user_tz":300},"id":"Rtorp3Y9mVB9","outputId":"f24d03bb-db8b-4d33-c42a-c099b112f1fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/Image Domain\n","BasicEncoderRun.ipynb  \u001b[0m\u001b[01;34mInDomain\u001b[0m/   TestEncoderRun.ipynb\n","\u001b[01;34mCavity_images\u001b[0m/         \u001b[01;34mOutDomain\u001b[0m/  \u001b[01;34mTrainingImages\u001b[0m/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/Image Domain\"\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"eyoIGEMDqsEA"},"source":["Importing the libraries!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uAt5HzCHiQ5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import PIL\n","from PIL import Image\n","import glob\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torchvision\n","from torchvision import transforms, datasets"]},{"cell_type":"markdown","metadata":{"id":"W6Hq6DDLqwNE"},"source":["Checking if we have GPU access!"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685774847800,"user":{"displayName":"VIDIT AGRAWAL","userId":"17500259214364609360"},"user_tz":300},"id":"XTzy1ywsqv9e","outputId":"1d8ceadc-dd51-451f-fac8-d4b0de12de1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device we are running on is mps\n"]}],"source":["device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","print(\"Device we are running on is\", device)"]},{"cell_type":"markdown","metadata":{"id":"wc2rUq3lfBnL"},"source":["Preprocessing the images to have a size of 1024, 1024 for now because I don't know what else to do. Also loading the training images as a tensor.\n","\n","TO DO: Make this into a function which I can call again later"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104796,"status":"ok","timestamp":1685774952594,"user":{"displayName":"VIDIT AGRAWAL","userId":"17500259214364609360"},"user_tz":300},"id":"yy-U4qeGkKpd","outputId":"e1f6b721-5fbc-4368-d91d-1b680c06eb9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([95, 1, 1024, 1024])\n"]}],"source":["TRAIN_DOMAIN_IMAGE_PATH = '/content/drive/My Drive/Image Domain/Cavity_images/cavity_nome_underfocus_only/train'\n","IN_DOMAIN_IMAGE_PATH = '/content/drive/My Drive/Image Domain/Cavity_images/cavity_nome_overfocus_only/val'\n","OUT_DOMAIN_IMAGE_PATH = '/contentdrive/My Drive/Image Domain/Cavity_images/cavity_nome_underfocus_only/val'\n","\n","compression_vector_size = 64\n","image_list = []\n","image_vector = np.empty((95, 1024, 1024), dtype=float)\n","desired_height = 1024\n","desired_width = 1024\n","for filename in glob.glob(TRAIN_DOMAIN_IMAGE_PATH + \"/*.png\"):\n","    im=Image.open(filename)\n","    transform = transforms.Compose([\n","      transforms.Resize((desired_height, desired_width)),\n","      transforms.ToTensor()\n","    ])\n","    transformed_im = transform(im)\n","    image_list.append(transformed_im)\n","num_images = len(image_list)\n","num_channels , height, width = image_list[0].shape\n","image_tensor = torch.zeros((num_images, num_channels, height, width))\n","for i, image in enumerate(image_list):\n","  if(image.size() == (3, 1024, 1024)):\n","    image_tensor[i, 0, :, :] = torch.mean(image, dim=0)\n","  else:\n","    image_tensor[i, :, :, :] = image\n","print(image_tensor.size())\n","Image_load = torch.utils.data.DataLoader(image_tensor, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35Klw4C7srpN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wUMy91sPj7Ml"},"source":["Making the architecture for the autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1685775346697,"user":{"displayName":"VIDIT AGRAWAL","userId":"17500259214364609360"},"user_tz":300},"id":"3jrZ1Rk1j677","outputId":"11a18925-98b2-43d0-bc96-c1cc8d401ac4"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n  def __init__(self):\\n    super().__init__()\\n    \\n    self.encoder = torch.nn.Sequential(\\n        torch.nn.Linear(1024*1024, 16384),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(16384, 4096),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(4096, 1024),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(1024, 256),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(256, 64),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(64, 18),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(18, 9)\\n    )\\n\\n    self.decoder = torch.nn.Sequential(\\n        torch.nn.Linear(9, 18),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(18, 64),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(64, 256),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(256, 1024),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(1024, 4096),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(4096, 16384),\\n        torch.nn.ReLU(),\\n        torch.nn.Linear(16384, 1024*1024),\\n        torch.nn.Sigmoid()\\n    )\\n\\n  def forward(self, x):\\n    encoded = self.encoder(x)\\n    decoded = self.decoder(encoded)\\n    return decoded\\n'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["class Autoencoder(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    # Add more layers, Conv 2D Layers\n","    self.encoder = torch.nn.Sequential(\n","        torch.nn.Linear(1024*1024, 256),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(256, 128)\n","    )\n","    self.decoder = torch.nn.Sequential(\n","        torch.nn.Linear(128, 256),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(256, 1024*1024)\n","    )\n","\n","  def forward(self, x):\n","    encoded = self.encoder(x)\n","    # Look at the distribution of histograms, make 128 bins for the points and\n","    # then see what it looks like\n","    decoded = self.decoder(encoded)\n","    return decoded, encoded\n","'''\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.encoder = torch.nn.Sequential(\n","        torch.nn.Linear(1024*1024, 16384),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(16384, 4096),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(4096, 1024),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(1024, 256),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(256, 64),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(64, 18),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(18, 9)\n","    )\n","\n","    self.decoder = torch.nn.Sequential(\n","        torch.nn.Linear(9, 18),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(18, 64),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(64, 256),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(256, 1024),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(1024, 4096),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(4096, 16384),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(16384, 1024*1024),\n","        torch.nn.Sigmoid()\n","    )\n","\n","  def forward(self, x):\n","    encoded = self.encoder(x)\n","    decoded = self.decoder(encoded)\n","    return decoded\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsWfZg1Lc3vf"},"outputs":[],"source":["#  use gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# create a model from `AE` autoencoder class\n","# load it to the specified device, either gpu or cpu\n","model = Autoencoder().to(device)\n","\n","# create an optimizer object\n","# Adam optimizer with learning rate 1e-3\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# mean-squared error loss\n","criterion = torch.nn.MSELoss()\n","'''\n","model = Autoencoder()\n","loss_function = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1, weight_decay = 1e8)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"elapsed":382,"status":"error","timestamp":1685775172715,"user":{"displayName":"VIDIT AGRAWAL","userId":"17500259214364609360"},"user_tz":300},"id":"6bf2E22qwWnL","outputId":"591542aa-6a6f-49ec-f234-fb873675e1e8"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-bba2d347eb1f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# compute training reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# compute accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3282\u001b[0m             \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m         )\n\u001b[0;32m-> 3284\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m         warnings.warn(\n\u001b[1;32m   3286\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"]}],"source":["epochs = 10\n","outputs = []\n","loss = 0\n","for epoch in range(epochs):\n","  for batch_features in Image_load:\n","    batch_features = batch_features.view(-1, 1024*1024).to(device)\n","    optimizer.zero_grad()\n","    #Images created\n","    outputs = model(batch_features)\n","    # compute training reconstruction loss\n","    train_loss = criterion(outputs, batch_features)\n","    # compute accumulated gradients\n","    train_loss.backward()\n","    # perform parameter update based on current gradients\n","    optimizer.step()\n","    # add the mini-batch training loss to epoch loss\n","    loss += train_loss.item()\n","\n","  loss = loss / len(Image_load)\n","  print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wYESqajLDZHOvfebIuiG34eazJUbpkEG"},"executionInfo":{"elapsed":31819,"status":"ok","timestamp":1685775003963,"user":{"displayName":"VIDIT AGRAWAL","userId":"17500259214364609360"},"user_tz":300},"id":"WVkubZAk09qd","outputId":"a977580e-2e88-4e1c-8086-b3049783d017"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["plt.gray()\n","for i, item in enumerate(batch_features):\n","   print(\"Original Images!\")\n","   item = item.reshape((-1, 1024, 1024))\n","   plt.imshow(item[0].cpu())\n","   plt.show()\n","for i, item in enumerate(outputs):\n","   print(\"Reconstructed Images!\")\n","   item = item.reshape((-1, 1024, 1024))\n","   plt.imshow(item[0].cpu().detach().numpy())\n","   plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}
